<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Itamar Zimerman</title>

    <meta name="author" content="Itamar Zimerman">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Itamar Zimerman
                </p>
                <p>I am a PhD candidate in the <a href="https://en-exact-sciences.tau.ac.il/computer">Blavatnik School of Computer Science </a> at Tel Aviv University, advised by <a href="https://www.cs.tau.ac.il/~wolf/">Prof. Lior Wolf. </a> Additionally, I work as an AI research scientist at IBM Research.
		</p>
              
                <p style="text-align:center">
                  <a href="mailto:itamarzimm@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=01s_DpwAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/ItamarZimerman">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/itamar-zimerman-2b180613a/">Linkedin</a> &nbsp;
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/JonBarron.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/twiiterimage.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:15px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My research focuses on modern deep learning architectures, with a particular emphasis on improving their capabilities in language modeling, computer vision, and crypto applications. I am passionate about developing architectures and understanding their design principles, especially transformers, state-space layers and modern RNNs.
                </p>
                <h2> Selected Publications</h2>
                Representative papers are <span class="highlight">highlighted</span>. '*' indicates equal contribution.
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:10px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


            <tr onmouseout="pnf_stop()" onmouseover="pnf_start()">
            <td style="padding:10px;width:30%;vertical-align:middle">
            <div class="one">
            <div class="two" id='pnf_image'>
              <img src='images/LaS attention.png' width="200"></div>
            <img src='images/LaS attention.png' width="200">
            </div>
            <script type="text/javascript">
            function pnf_start() {
              document.getElementById('pnf_image').style.opacity = "1";
            }

            function pnf_stop() {
              document.getElementById('pnf_image').style.opacity = "0";
            }
            pnf_stop()
            </script>
            </td>
            <td style="padding:10px;width:70%;vertical-align:top">
            <a href="TODO">
            <span class="papertitle"><span class="highlight"> Viewing Transformers Through the Lens of Long Convolutions Layers</span></span>
            </a> <br>
            <strong>I. Zimerman</strong>, L. Wolf
            <p>
              We investigate the design principles that enable long-range layers, such as state-space models and Hyena, to outperform transformers on long-range tasks.
            </p>
        
            <em>ICML</em>, 2024
          
            </td>
      </tr>  
  
  <table style="width:100%;border:10px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


    <tr onmouseout="pnf_stop()" onmouseover="pnf_start()">
    <td style="padding:10px;width:30%;vertical-align:middle">
    <div class="one">
    <div class="two" id='pnf_image'>
      <img src='images/threatModel.png' width="200"></div>
    <img src='images/threatModel.png' width="200">
    </div>
    <script type="text/javascript">
    function pnf_start() {
      document.getElementById('pnf_image').style.opacity = "1";
    }

    function pnf_stop() {
      document.getElementById('pnf_image').style.opacity = "0";
    }
    pnf_stop()
    </script>
    </td>
    <td style="padding:10px;width:70%;vertical-align:top">
    <a href="TODO">
    <span class="papertitle">Converting Transformers to Polynomial Form for Secure Inference Over Homomorphic Encryption</span>
    </a> <br>
    <strong>I. Zimerman</strong>,
    M. Baruch, N. Drucker, G. Ezov, O. Soceanu, L. Wolf
    <p>
      We introduce the first polynomial transformer, which is based on a novel alternative to softmax and layer normalization. This breakthrough enables the application of transformers on encrypted data for the first time.
    </p>
 
    <em>ICML</em>, 2024
  
    </td>
</tr> 

<table style="width:100%;border:10px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


  <tr onmouseout="pnf_stop()" onmouseover="pnf_start()">
  <td style="padding:10px;width:30%;vertical-align:middle">
  <div class="one">
  <div class="two" id='pnf_image'>
    <img src='images/mamba-computation_larger.jpg' width="200"></div>
  <img src='images/mamba-computation_larger.jpg' width="200">
  </div>
  <script type="text/javascript">
  function pnf_start() {
    document.getElementById('pnf_image').style.opacity = "1";
  }

  function pnf_stop() {
    document.getElementById('pnf_image').style.opacity = "0";
  }
  pnf_stop()
  </script>
  </td>
  <td style="padding:10px;width:70%;vertical-align:top">
  <a href="TODO">
  <span class="papertitle">	
    <span class="highlight"> The Hidden Attention of Mamba Models</span> </span>
  </a> <br>
  A. Ali*, <strong>I. Zimerman*</strong>, L. Wolf
  <p>
    We establish the connection between S6 layers and causal self-attention, demonstrating that both rely on a data-control linear operator, which can be represented through attention matrices. Using these matrices, we introduce the first explainability tools for Mamba models.
  </p>
    <em>arXiv, 2024</em>

  </td>
</tr>

<table style="width:100%;border:10px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


  <tr onmouseout="pnf_stop()" onmouseover="pnf_start()">
  <td style="padding:10px;width:30%;vertical-align:middle">
  <div class="one">
  <div class="two" id='pnf_image'>
    <img src='images/2DSSM.png' width="200"></div>
  <img src='images/2DSSM.png' width="200">
  </div>
  <script type="text/javascript">
  function pnf_start() {
    document.getElementById('pnf_image').style.opacity = "1";
  }

  function pnf_stop() {
    document.getElementById('pnf_image').style.opacity = "0";
  }
  pnf_stop()
  </script>
  </td>
  <td style="padding:10px;width:70%;vertical-align:top">
  <a href="TODO">
  <span class="papertitle">	
    <span class="highlight"> A 2-Dimensional State Space Layer for Spatial Inductive Bias</span> </span>
  </a> <br>
  E. Baron*, <strong>I. Zimerman*</strong>, L. Wolf
  <p>
    2D-SSM is a spatial layer that extends S4 into multi-axis data and is parametrized by 2-D recursion. Empirically, it enhances the performance of various ViTs and ConvNeXt backbones. Theoretically, our layer is more expressive than previous extensions.
  </p>
  <em>ICLR</em>, 2024

  </td>
</tr>

<table style="width:100%;border:10px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


  <tr onmouseout="pnf_stop()" onmouseover="pnf_start()">
  <td style="padding:10px;width:30%;vertical-align:middle">
  <div class="one">
  <div class="two" id='pnf_image'>
    <img src='images/hyena2dFinal.jpg' width="200"></div>
  <img src='images/hyena2dFinal.jpg' width="200">
  </div>
  <script type="text/javascript">
  function pnf_start() {
    document.getElementById('pnf_image').style.opacity = "1";
  }

  function pnf_stop() {
    document.getElementById('pnf_image').style.opacity = "0";
  }
  pnf_stop()
  </script>
  </td>
  <td style="padding:10px;width:70%;vertical-align:top">
  <a href="TODO">
  <span class="papertitle">	
    Multi-Dimensional Hyena for Spatial Inductive Bias</span>
  </a> <br>
  <strong>I. Zimerman</strong>, L. Wolf
  <p>
    We present Hyena N-D, a theoretically grounded extension of the Hyena layer to multi-axis sequence modeling. We show that when incorporated into a ViT instead of an attention mechanism, it results in a data- and memory-efficient model.
  </p>
  <em>AISTATS</em>, 2024

  </td>
</tr>

<table style="width:100%;border:10px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


  <tr onmouseout="pnf_stop()" onmouseover="pnf_start()">
  <td style="padding:10px;width:30%;vertical-align:middle">
  <div class="one">
  <div class="two" id='pnf_image'>
    <img src='images/focus_full_arch.png' width="200"></div>
  <img src='images/focus_full_arch.png' width="200">
  </div>
  <script type="text/javascript">
  function pnf_start() {
    document.getElementById('pnf_image').style.opacity = "1";
  }

  function pnf_stop() {
    document.getElementById('pnf_image').style.opacity = "0";
  }
  pnf_stop()
  </script>
  </td>
  <td style="padding:10px;width:70%;vertical-align:top">
  <a href="TODO">
  <span class="papertitle">	
    <span class="highlight"> Focus Your Attention (with Adaptive IIR Filters) </span></span>
  </a> <br>
  S. Lutati, <strong>I. Zimerman</strong>, L. Wolf
  <p>
    Focus is a new architecture for LMs built on adaptive IIR filters. We show that second-order IIR filters are theoretically more expressive than state-space models, yet still stable. We boost the performance of these filters by making them adaptive, with the first global-convolution-based hypernetwork.
  </p>
  <em>EMNLP</em>, 2023 <span style="color: red;">(Oral presentation)</span>

  </td>
</tr>
            
<table style="width:100%;border:10px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


  <tr onmouseout="pnf_stop()" onmouseover="pnf_start()">
  <td style="padding:10px;width:30%;vertical-align:middle">
  <div class="one">
  <div class="two" id='pnf_image'>
    <img src='images/rnn_comp_no_caption.png' width="200"></div>
  <img src='images/rnn_comp_no_caption.png' width="200">
  </div>
  <script type="text/javascript">
  function pnf_start() {
    document.getElementById('pnf_image').style.opacity = "1";
  }

  function pnf_stop() {
    document.getElementById('pnf_image').style.opacity = "0";
  }
  pnf_stop()
  </script>
  </td>
  <td style="padding:10px;width:70%;vertical-align:top">
  <a href="TODO">
  <span class="papertitle">	
    Decision S4: Efficient Sequence-Based RL via State Spaces Layers</span>
  </a> <br>
  S. Bar-David*, <strong>I. Zimerman*</strong>, E. Nachmani, L. Wolf
  <p>
    We explore the application of the state-space layers in offline and online RL, highlighting its superiority in handling long-range dependencies and its improved efficiency over transformers.
  </p>
  <em>ICLR</em>, 2023

  </td>
</tr>

<table style="width:100%;border:10px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


  <tr onmouseout="pnf_stop()" onmouseover="pnf_start()">
  <td style="padding:10px;width:30%;vertical-align:middle">
  <div class="one">
  <div class="two" id='pnf_image'>
    <img src='images/training_scheme.png' width="200"></div>
  <img src='images/training_scheme.png' width="200">
  </div>
  <script type="text/javascript">
  function pnf_start() {
    document.getElementById('pnf_image').style.opacity = "1";
  }

  function pnf_stop() {
    document.getElementById('pnf_image').style.opacity = "0";
  }
  pnf_stop()
  </script>
  </td>
  <td style="padding:10px;width:70%;vertical-align:top">
  <a href="TODO">
  <span class="papertitle">	
    Recovering AES Keys with a Deep Cold Boot Attack</span>
  </a> <br>
  <strong>I. Zimerman*</strong>, E. Nachmani*, L. Wolf
  <p>
    Here we present a DL-based side-channel attack on AES keys. Our method combines a SAT solver with neural tools from the field of error correction coding to enhance attack effectiveness.
  </p>
  <em>ICML</em>, 2021

  </td>
</tr>
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:05px">
                <br>
                <p style="text-align:middle;font-size: xx-small;">
                  This page design is based on a template by <a href="https://drive.google.com/file/d/13rVuJpcytRdLYCnKpq46g7B7IzSrPQ2P/view?usp=sharing" style="text-align:middle;font-size: xx-small;"> Jon Barron.
                </p>
              </td>
            </tr> 
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
